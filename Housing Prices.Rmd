---
title: "Housing Prices Prediction"
output: html_notebook
---

For this project I am interested in predicting housing prices based on typical
features like number rooms, distance to the ocean, and location. This data set
is from houses in California and was retrieved from Kaggle. 

First thing is to load in the data as well as the packages I will be using. 

```{r}
rm(list = ls())

setwd("C:/Users/cayde/OneDrive/Data Science/R Projects/Housing Prices")

data <- read.csv("housing.csv")

library(dplyr)
library(ggplot2)

library(reshape2) #cormelt
library(Hmisc) #Hist.data.frame
library(car) #VIF
library(randomForest)

```

Next I want to inspect the data. One of the most important aspects is how clean 
or complete the data set is. 

```{r}
sapply(data, function(x){sum(is.na(x))}) # checking for NA's
missing_data <- data[is.na(data$total_bedrooms),]
```

Fortunately this data looks quite clean, only one column is missing info and less
than 1% of observations are missing. 

Although highly unlikely I want to see if the missing data differs systematically
from the complete data. Is there any reason why certain houses may have missing
data in the data set? 

Since most of the data is continuous, I use a t.test to compare the means between
the observations with full data and those with missing data. 

```{r}
numeric_vars <- names(data[sapply(data, is.numeric)]) #getting a list of the numeric variables
numeric_vars <- numeric_vars[!numeric_vars %in% "total_bedrooms"] # excluding the variable with missing data

# running a t-test for every variable
for(variable in numeric_vars){
  test <- t.test(data[,variable], missing_data[,variable])
  print(test)
}
```

None of the p values reach statistical significance. This indicates that there
are likely no systematic differences for the observations that are missing 
bedroom data. I thus feel more confident about removing these observations. I do
this to create a perfectly clean data set as NA's can be very annoying. 

```{r}
data <- na.omit(data) 
```

Taking a look at the data everything seems in order. The max median income and 
house value suggest that their may be an artificial cap in the data. 

```{r}
for(variable in numeric_vars){
  print(variable)
  print(summary(data[,variable]))
}

rm(numeric_vars)
```



The first method I would like to use is linear regression. Although there are 
more complicated and fancier models I am a big fan of Ockam's Razor, which states
that causes should not be multiplied beyond their necessity, or in laymen's terms,
don't make things more complicated then they need to be. 

A Linear Regression, however, makes certain assumptions about the data. One of those
assumptions is that the data is normally distributed. Using Histograms I can have 
a visual check to see if this is true. 

```{r}

hist.data.frame(data)

```

Some of the data do not look normally distributed
Let's take a closer look.

```{r}
# hist.data.frame(select(data, c("total_rooms", "total_bedrooms", "population", "households")))
hist(data$total_rooms)
hist(data$total_bedrooms)
hist(data$population)
hist(data$households)
```

These variables appear to be exponentially distributed
Let's log transform them to see if we get a distribution that looks more normal.

```{r}
data$total_rooms <- log(data$total_rooms)
data$total_bedrooms <- log(data$total_bedrooms)
data$population <- log(data$population)
data$households <- log(data$households)

#hist.data.frame(select(data, c("total_rooms", "total_bedrooms", "population", "households")))
hist(data$total_rooms)
hist(data$total_bedrooms)
hist(data$population)
hist(data$households)
```

These look much better! Let's make a couple of other minor adjustments to the data.

```{r}
# There are only 5 houses on islands, so that variable is dropped
table(data$ocean_proximity)
data <- data[data$ocean_proximity != "ISLAND",]

# Converting the geographic variable into a factor
class(data$ocean_proximity)
data$ocean_proximity <- as.factor(data$ocean_proximity)
```

Now that the data is cleaned and transformed we can split the data into training
and testing sets. Splitting the data this way ensures we do not 'overfit' the model
or chase any noise. By keeping a section of the data reserved, we can better ensure
that the improvements in accuracy are not due to the model being overtrained. 

```{r}
set.seed(38483)

assignment <- sample(c("train", "test"), nrow(data), replace = TRUE, prob = c(.8, .2))

train <- data[assignment == "train",]
test <- data[assignment == "test",]

rm(assignment)
```

Now that we have two sets, let's estimate a linear model on the training set.

```{r}
lin_mod_full <- lm(median_house_value ~ ., train)
summary(lin_mod_full)
```

Running Linear models requires certain assumptions about the data. I have already
dealt with the normality of the data, and I have no reason to believe the 
observations are not independent of each other. However, now that I have estimated
the model I need to check for multicolinearity, homoskedasticity, and normality 
of errors.

Looking at the coefficients of the linear model, I can see that the total number
of rooms is negatively correlated with the price, which seems backwards; how does
having more rooms make a house cost less? There is also a variable named total 
number of bedrooms, which is not only positively correlated, but sounds very 
similar to total number of rooms. The variables 'total_rooms' and 'total_bedrooms
are likely highly correlated, thus causing strange behavior for the coefficients.
Let's run some tests and find out.

```{r}
cormat <- round(cor(data[,sapply(data, is.numeric)]), 2)
cormat

vif(lin_mod_full)
```

The covariance matrix allows us to see how the covariates or features of our data
are correlated with each other. Calculating a variation inflaction factor score (VIF)
we see that total rooms and number of bedrooms are near 5 and 6. This is normally
the cutoff point for collinearity. 

I remove the number of bedrooms variable and reestimate the model

```{r}
lin_mod_rooms<- lm(median_house_value ~ housing_median_age + total_rooms + 
                     population + median_income + ocean_proximity, train)

summary(lin_mod_rooms)
vif(lin_mod_rooms)

```

The new model has significantly less multicollineartiy 
If I was interested in the impact of the particular variables of the model I would
likely go with the reduced model as the coefficients are more stable, however
I am more interested in predictive power, and this new model has less predictive
power.

Let's check for the other assumptions of linear regression, namely normality of 
errors and homoskedasticity.

```{r}
# Normality of Errors 
#####################

qqnorm(scale(lin_mod_rooms$residuals))
qqline(scale(lin_mod_rooms$residuals))
```

The QQ plot shows compares the distribution of the residuals to a normal 
distribution. Here we can see that although many of the residuals fall on a 
straight line, as the residuals get larger their values become more extreme 
compared to a normal distribution. The errors are not normally distributed, and
in this case the model is significantly overpredict  the values for a large 
quantity of data

```{r}
#Homoskedasticity
plot(lin_mod_rooms$fitted.values, lin_mod_rooms$residuals)
```

Plotting the residuals and fitted values against each other we can see that 
there does appear to be a relationship between the variables, thus violating
the assumption of Homoskedasticity.  There is also a very strong diagonal line 
on the plot. Let's check the data for peculiarities. 



```{r}
sum(train$median_house_value == max(train$median_house_value))
```

There are 777 houses at maximum price "500001", this suggests a 'ceiling' in 
our data and creates artificiality
Let's remove those values and retrain the model. 

```{r}
train <- train[train$median_house_value != max(train$median_house_value),]

lin_mod_rooms<- lm(median_house_value ~ housing_median_age + total_rooms + 
                     population + median_income + ocean_proximity, train)
```

Removing the values get's rid of the strong diagonal line but the errors are 
still not normally distributed

```{r}
plot(lin_mod_rooms$fitted.values, lin_mod_rooms$residuals)

qqnorm(scale(lin_mod_rooms$residuals))
qqline(scale(lin_mod_rooms$residuals))
```


Although there appears to be a negative correlation between the residuals
this relationship is not statistically significant. Thus we may have homoskedasticity

```{r}
cor.test(lin_mod_rooms$fitted.values, lin_mod_rooms$residuals)
```

Now that we've trained a model let's apply it to our test data. 
```{r}
pred_test <- predict(lin_mod_rooms, test)

rss <- sum((test$median_house_value - pred_test)^2)
tss <- sum((test$median_house_value - mean(test$median_house_value))^2)

r_squared <- 1 - (rss/tss)
r_squared
```

The linear model does predict 63% of the variance on 
the test data. However, some of the assumptions of linear regression do not hold, 
particularly normality of errors. And despite use log-transforming some of the variables
they are still not perfectly normal as they have long tails. 

Let's try another model, as I think we can improve on 63% accuracy.

Random forest models make less assumptions about the structure of the data
While there is a risk of overfitting, we over already split the data into training
and testing sets, so if we are overfitting we will know.
Random forests are not as easy to interpret as a linear regression models,
in which the coefficients are a straight-forward linear interpretation
However, if the goal is prediction, the Random Forest Model is often quite good

```{r}

rf_model <- randomForest(median_house_value ~ ., data = train)
print(rf_model)

rf_test <- predict(rf_model, newdata = test)

rss_rf <- sum((test$median_house_value - rf_test)^2)
tss_rf <- sum((test$median_house_value - mean(test$median_house_value))^2)

r_squared_rf <- 1 -(rss_rf/tss_rf)
r_squared_rf
```

The predictive power of random forest model is significantly greater than the
linear model explaining about 80% of the data, nearly 20% more the the linear
model.

Let's try and tune our parameters, notably the number of features. When a random
forest creates it's decision tree it does so by choosing from x number of variables 
or features. We can set x to whatever value we like  as long as it is less than
the total number of variables. Lets create a loop and estimate random forest models
for 2 to 7 features. 

```{r}


for(features in 2:7){
  assign(paste0("rf_test_", features), randomForest(median_house_value ~ ., data = train, mtry = features))
}
print(rf_test_2)
print(rf_test_3)
print(rf_test_4)
print(rf_test_5)
print(rf_test_6)
print(rf_test_7)
```

Adjusting the number of features the model selects from it makes little difference, 
all models are predicting about 80% of the variance.

Overall I would use a random forest model for this project. Predicting 80% of the
variance with about a dozen variables is quite substantial. 

The most obvious way I would improve this model is by transforming the longitude 
and latitude variables. While both these variables are useful, the real information
lies in the combination of these values. I conduct an unsupervised k-means clustering
to try and cluster these coordinates into geographic regions. Having a set of geographic
regions in the data will probably be more helpful than just the coordinates. 

First I create an elbow plot the plots the Within-Cluster Sum of Squares (WCSS)
against the number of points. The idea is to see where adding more clusters only
gives marginal returns. 


```{r}
wcss <- c()

for (i in 1:15) {
  kmeans_obj <- kmeans(data[,c("longitude", "latitude")], centers = i)
  wcss[i] <- kmeans_obj$tot.withinss
}

# plot the elbow plot
plot(1:15, wcss, type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters", ylab = "WCSS")

# add a vertical line at the "elbow" point
elbow <- which(diff(wcss) <= mean(diff(wcss))) + 1
abline(v = elbow, col = "red", lty = 2)
```

Although mathematically the biggest drop in WCSS is at 2 clusters, California
is too large and diverse of a state to only have 2 relevant locations for housing
prices. I go further along the elbow plot until the drop in WCSS is flat. The
graph slowly drops to 8, which seems more reasonable. 

```{r}
kmeans_cluster <- kmeans(data[,c("longitude", "latitude")], centers = 8)

data$area <-  as.factor(kmeans_obj$cluster)
table(data$area)

ggplot(data, aes(x=longitude, y=latitude, color = area)) +
  geom_point()
```

A simple scatter plot of the latiude and longitude does a surprisingly good job
of showing California. The Major cities of San Francisco, Sacramento, Los Angeles,
and San Diego all have distinct colors, with Los Angeles being divided into three
colors. 

Although this K-means cluster was designed to make the latitude and longitude 
more meaningful I will still keep these values in the data set. The Random Forest
works by selecting the feature that maximizes information gain, if the new clusters
do a better job at predicting prices than the latitude and longitude the model 
will simply select the area when confronted with a choice. In other words 
Random forest models are robust to meaningless data because it can always ignore
a feature in favor of a variable that provides more information gain.

Since I have added new data to the data set I need to resplit my data to make
sure the new feature shows up in the training and test set. I simply set the 
same seed and re-assign

```{r}
set.seed(38483)

assignment <- sample(c("train", "test"), nrow(data), replace = TRUE, prob = c(.8, .2))

train <- data[assignment == "train",]
test <- data[assignment == "test",]

rm(assignment)

rf_model <- randomForest(median_house_value ~ ., data = train)
print(rf_model)

rf_test <- predict(rf_model, newdata = test)

rss_rf <- sum((test$median_house_value - rf_test)^2)
tss_rf <- sum((test$median_house_value - mean(test$median_house_value))^2)

r_squared_rf <- 1 -(rss_rf/tss_rf)
r_squared_rf
```

Assigning house to areas using their coordinates resulted predicting 82% of 
the variance, a 4% increase.

Overall I would likely use a Random Forest Model on this data rather than a linear
regression. The Random Forest Model has significantly more accuracy at 82%, a
considerable feet considering that the square footage of the house, one of the 
main predictors of price, was not in the data set. 

